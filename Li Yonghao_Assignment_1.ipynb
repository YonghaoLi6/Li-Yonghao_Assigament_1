{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Given a fully connected Neural Network as follows:\n",
    "1. Input (x1,x2): 2 nodes\n",
    "2. First hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation \n",
    "function\n",
    "3. Second hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation \n",
    "function\n",
    "4. Output (predict): 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,10),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(10,10),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(10,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Generate the input date (x1,x2) \\in [0,1] drawn from a uniform random distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0845, 0.4541],\n",
       "        [0.1000, 0.0077],\n",
       "        [0.5280, 0.7234],\n",
       "        [0.4587, 0.0669],\n",
       "        [0.2261, 0.5997],\n",
       "        [0.4660, 0.7235],\n",
       "        [0.8067, 0.8530],\n",
       "        [0.2272, 0.8020],\n",
       "        [0.7508, 0.5379],\n",
       "        [0.7824, 0.7798],\n",
       "        [0.6293, 0.3630],\n",
       "        [0.1827, 0.5907],\n",
       "        [0.8485, 0.8222],\n",
       "        [0.3559, 0.1848],\n",
       "        [0.0068, 0.0971],\n",
       "        [0.3872, 0.9265],\n",
       "        [0.1222, 0.7810],\n",
       "        [0.1364, 0.1409],\n",
       "        [0.8931, 0.9876],\n",
       "        [0.2827, 0.9217],\n",
       "        [0.8821, 0.7959],\n",
       "        [0.3673, 0.1539],\n",
       "        [0.1380, 0.5071],\n",
       "        [0.3385, 0.0753],\n",
       "        [0.9487, 0.4769],\n",
       "        [0.9868, 0.1300],\n",
       "        [0.0333, 0.1226],\n",
       "        [0.4721, 0.7510],\n",
       "        [0.2858, 0.4291],\n",
       "        [0.6794, 0.8045],\n",
       "        [0.4923, 0.0603],\n",
       "        [0.8873, 0.4778],\n",
       "        [0.2597, 0.6872],\n",
       "        [0.6658, 0.2171],\n",
       "        [0.8340, 0.9294],\n",
       "        [0.7972, 0.3764],\n",
       "        [0.2811, 0.1619],\n",
       "        [0.9062, 0.2838],\n",
       "        [0.5079, 0.6865],\n",
       "        [0.1270, 0.3869],\n",
       "        [0.4750, 0.8639],\n",
       "        [0.9211, 0.9880],\n",
       "        [0.8772, 0.0553],\n",
       "        [0.0217, 0.9036],\n",
       "        [0.1407, 0.0470],\n",
       "        [0.3947, 0.1698],\n",
       "        [0.9547, 0.4341],\n",
       "        [0.5348, 0.3199],\n",
       "        [0.1561, 0.5071],\n",
       "        [0.5581, 0.3134],\n",
       "        [0.6263, 0.0653],\n",
       "        [0.4066, 0.0523],\n",
       "        [0.5966, 0.0184],\n",
       "        [0.8595, 0.9634],\n",
       "        [0.4317, 0.8726],\n",
       "        [0.4365, 0.6826],\n",
       "        [0.9408, 0.6336],\n",
       "        [0.6090, 0.8739],\n",
       "        [0.0314, 0.0205],\n",
       "        [0.6382, 0.5818],\n",
       "        [0.2460, 0.0776],\n",
       "        [0.6769, 0.1215],\n",
       "        [0.7538, 0.6918],\n",
       "        [0.1273, 0.9865],\n",
       "        [0.9311, 0.2016],\n",
       "        [0.1739, 0.0112],\n",
       "        [0.8973, 0.0693],\n",
       "        [0.0989, 0.9356],\n",
       "        [0.9126, 0.2771],\n",
       "        [0.1234, 0.2313],\n",
       "        [0.3430, 0.6786],\n",
       "        [0.1114, 0.0474],\n",
       "        [0.6362, 0.7745],\n",
       "        [0.9230, 0.2709],\n",
       "        [0.9985, 0.1201],\n",
       "        [0.1790, 0.8009],\n",
       "        [0.5338, 0.2567],\n",
       "        [0.9327, 0.1824],\n",
       "        [0.0383, 0.6972],\n",
       "        [0.1647, 0.6561],\n",
       "        [0.7965, 0.4152],\n",
       "        [0.3155, 0.9963],\n",
       "        [0.3932, 0.5413],\n",
       "        [0.2939, 0.2658],\n",
       "        [0.3968, 0.0286],\n",
       "        [0.3336, 0.1113],\n",
       "        [0.8668, 0.1026],\n",
       "        [0.8341, 0.2730],\n",
       "        [0.2746, 0.9444],\n",
       "        [0.1021, 0.2364],\n",
       "        [0.3114, 0.0745],\n",
       "        [0.9781, 0.5277],\n",
       "        [0.6055, 0.7161],\n",
       "        [0.8866, 0.4060],\n",
       "        [0.2360, 0.5206],\n",
       "        [0.4651, 0.8095],\n",
       "        [0.2764, 0.9962],\n",
       "        [0.0639, 0.4392],\n",
       "        [0.4416, 0.2205],\n",
       "        [0.1803, 0.0276]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = torch.distributions.Uniform(low=0, high=1)\n",
    "x = sampler.sample((100,2))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Generate the labels y = (x1*x1+x2*x2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0670e-01, 5.0284e-03, 4.0107e-01, 1.0743e-01, 2.0540e-01, 3.7027e-01,\n",
       "        6.8917e-01, 3.4741e-01, 4.2650e-01, 6.1016e-01, 2.6390e-01, 1.9113e-01,\n",
       "        6.9800e-01, 8.0425e-02, 4.7352e-03, 5.0413e-01, 3.1245e-01, 1.9231e-02,\n",
       "        8.8650e-01, 4.6476e-01, 7.0576e-01, 7.9289e-02, 1.3808e-01, 6.0136e-02,\n",
       "        5.6368e-01, 4.9536e-01, 8.0743e-03, 3.9340e-01, 1.3290e-01, 5.5439e-01,\n",
       "        1.2298e-01, 5.0783e-01, 2.6985e-01, 2.4521e-01, 7.7962e-01, 3.8861e-01,\n",
       "        5.2615e-02, 4.5084e-01, 3.6459e-01, 8.2902e-02, 4.8593e-01, 9.1232e-01,\n",
       "        3.8626e-01, 4.0845e-01, 1.1007e-02, 9.2333e-02, 5.4997e-01, 1.9417e-01,\n",
       "        1.4074e-01, 2.0488e-01, 1.9824e-01, 8.4020e-02, 1.7813e-01, 8.3347e-01,\n",
       "        4.7391e-01, 3.2822e-01, 6.4327e-01, 5.6733e-01, 7.0137e-04, 3.7293e-01,\n",
       "        3.3281e-02, 2.3647e-01, 5.2337e-01, 4.9467e-01, 4.5381e-01, 1.5188e-02,\n",
       "        4.0496e-01, 4.4260e-01, 4.5484e-01, 3.4376e-02, 2.8905e-01, 7.3342e-03,\n",
       "        5.0232e-01, 4.6262e-01, 5.0572e-01, 3.3677e-01, 1.7541e-01, 4.5159e-01,\n",
       "        2.4381e-01, 2.2879e-01, 4.0343e-01, 5.4613e-01, 2.2381e-01, 7.8510e-02,\n",
       "        7.9116e-02, 6.1845e-02, 3.8096e-01, 3.8514e-01, 4.8369e-01, 3.3150e-02,\n",
       "        5.1265e-02, 6.1754e-01, 4.3973e-01, 4.7543e-01, 1.6336e-01, 4.3580e-01,\n",
       "        5.3446e-01, 9.8476e-02, 1.2179e-01, 1.6628e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x[:,0]*x[:,0] + x[:,1]*x[:,1])/2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    " Implement a loss function L = (predict-y)^2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(predict,y):\n",
    "    return torch.sum((predict-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Use batch size of 1, that means feed data one point at a time into network and compute \n",
    "the loss. Do one time forward propagation with one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0845, 0.4541])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1067)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.6834869\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net1.parameters(), lr=0.05)\n",
    "optimizer.zero_grad()\n",
    "prediction = net1(x[0])\n",
    "loss = L(prediction, y[0])\n",
    "loss.backward()\n",
    "print(\"Loss = \" + str(loss.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    " Compute the gradients using pytorch autograd:\n",
    " \n",
    "a. dL/dw, dL/db\n",
    "\n",
    "b. Print these values into a text file: torch_autograd.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=10, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = net1[0]\n",
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=10, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer1 = net1[2]\n",
    "hidden_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=1, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer2 = net1[4]\n",
    "hidden_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.68349\n",
      "y_prediction:[-0.72003]\n",
      "Input_layer w_gradient: [[-1.00e-05 -8.00e-05]\n",
      " [ 2.10e-04  1.13e-03]\n",
      " [ 9.00e-05  5.00e-04]\n",
      " [-1.30e-04 -6.80e-04]\n",
      " [-2.80e-04 -1.52e-03]\n",
      " [ 6.80e-04  3.67e-03]\n",
      " [-1.13e-03 -6.05e-03]\n",
      " [ 2.10e-04  1.12e-03]\n",
      " [ 1.10e-03  5.93e-03]\n",
      " [ 9.00e-04  4.85e-03]]\n",
      "Input_layer b_gradient: [-0.00017  0.00248  0.0011  -0.0015  -0.00336  0.00809 -0.01333  0.00246\n",
      "  0.01305  0.01068]\n",
      "Hidden_layer_1 weight_gradient: [[ 0.02321  0.02342  0.0252   0.03736  0.03658  0.03411  0.0398   0.02897\n",
      "   0.02532  0.01899]\n",
      " [ 0.02352  0.02373  0.02553  0.03786  0.03706  0.03456  0.04032  0.02935\n",
      "   0.02565  0.01924]\n",
      " [ 0.03394  0.03424  0.03684  0.05463  0.05348  0.04988  0.05819  0.04236\n",
      "   0.03702  0.02776]\n",
      " [ 0.0408   0.04117  0.0443   0.06568  0.0643   0.05997  0.06997  0.05093\n",
      "   0.04451  0.03338]\n",
      " [ 0.0046   0.00464  0.00499  0.0074   0.00724  0.00675  0.00788  0.00574\n",
      "   0.00501  0.00376]\n",
      " [-0.01027 -0.01036 -0.01115 -0.01653 -0.01618 -0.01509 -0.01761 -0.01282\n",
      "  -0.0112  -0.0084 ]\n",
      " [ 0.03103  0.03131  0.03368  0.04995  0.0489   0.0456   0.0532   0.03873\n",
      "   0.03385  0.02538]\n",
      " [-0.01203 -0.01214 -0.01307 -0.01937 -0.01897 -0.01769 -0.02064 -0.01502\n",
      "  -0.01313 -0.00985]\n",
      " [-0.0257  -0.02594 -0.02791 -0.04138 -0.04051 -0.03778 -0.04408 -0.03208\n",
      "  -0.02804 -0.02103]\n",
      " [ 0.02161  0.0218   0.02346  0.03478  0.03405  0.03176  0.03705  0.02697\n",
      "   0.02357  0.01768]]\n",
      "Hidden_layer_1 bias_gradient: [ 0.05976  0.06056  0.08739  0.10507  0.01183 -0.02644  0.07989 -0.03099\n",
      " -0.06619  0.05564]\n",
      "Hidden_layer_2 weight_gradient: [-0.73496 -0.54626 -0.87168 -0.9232  -0.63875 -0.87525 -0.79051 -0.6599\n",
      " -0.84924 -1.06119]\n",
      "Hidden_layer_2 bias_gradient: [-1.65347]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss = \" + str(np.round(loss.item(),5)))\n",
    "print(\"y_prediction:\" + str(np.round(prediction.tolist(),5)))\n",
    "print(\"Input_layer w_gradient: \" + str(np.round(input_layer.weight.grad.tolist(),5)))\n",
    "print(\"Input_layer b_gradient: \" + str(np.round(input_layer.bias.grad.tolist(),5)))\n",
    "print(\"Hidden_layer_1 weight_gradient: \" + str(np.round(hidden_layer1.weight.grad.tolist(),5)))\n",
    "print(\"Hidden_layer_1 bias_gradient: \" + str(np.round(hidden_layer1.bias.grad.tolist(),5)))\n",
    "print(\"Hidden_layer_2 weight_gradient: \" + str(np.round(hidden_layer2.weight.grad.tolist()[0],5)))\n",
    "print(\"Hidden_layer_2 bias_gradient: \" + str(np.round(hidden_layer2.bias.grad.tolist(),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Implement the forward propagation and backpropagation algorithm from scratch, \n",
    "without using pytorch autograd, compute the gradients using your implementation\n",
    "\n",
    "a. dL/dw, dL/db\n",
    "\n",
    "b. Print these values into a text file: my_autograd.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = input_layer.weight.t().detach().numpy()\n",
    "b1 = input_layer.bias.detach().numpy()\n",
    "w2 = hidden_layer1.weight.t().detach().numpy()\n",
    "b2 = hidden_layer1.bias.detach().numpy()\n",
    "w3 = hidden_layer2.weight.t().detach().numpy()\n",
    "b3 = hidden_layer2.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2655839 ,  0.65283793,  0.6149356 , -0.38900924,  0.58017904,\n",
       "        -0.5895183 ,  0.07981533,  0.14305836, -0.00192529,  0.32974452],\n",
       "       [-0.31671306, -0.3248271 ,  0.43579274,  0.46074122,  0.46709436,\n",
       "        -0.38845772,  0.2446639 , -0.26276284,  0.4096616 , -0.53121144]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0845, 0.4541], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = x[0].numpy()\n",
    "x0 = np.round(x0,4)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10669841]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.ones([1,1])\n",
    "a[0][0]=y[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1067]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0 = a\n",
    "y0 = np.round(y0,4)\n",
    "y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = x0.dot(w1)+b1\n",
    "hidden1_sigmoid = 1.0 / (1.0 + np.exp(-hidden1))\n",
    "hidden2 = hidden1_sigmoid.dot(w2)+b2\n",
    "hidden2_sigmoid = 1.0 / (1.0 + np.exp(-hidden2))\n",
    "prediction_ = hidden2_sigmoid.dot(w3)+b3\n",
    "loss_forward = np.square(prediction_ - y0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_prediction_ = 2.0 * (prediction_ - y0)\n",
    "\n",
    "dt = float(dy_prediction_)\n",
    "dw3 = np.dot(hidden2_sigmoid.T, dt)\n",
    "db3 = np.ones(1).dot(dt)\n",
    "\n",
    "dt = np.dot(dt, w3.T)*hidden2_sigmoid*(1-hidden2_sigmoid)\n",
    "dw2 = np.dot(hidden1_sigmoid.reshape(len(hidden1_sigmoid),1), dt)\n",
    "db2 = np.ones(1).dot(dt)\n",
    "\n",
    "dt = np.dot(dt, w2.T)*hidden1_sigmoid*(1-hidden1_sigmoid)\n",
    "dw1 = np.dot(x0.reshape(len(x0),1), dt)\n",
    "db1 = np.ones(1).dot(dt)\n",
    "\n",
    "w1 -= 0.05 * dw1 \n",
    "w2 -= 0.05 * dw2\n",
    "w3 -= 0.05 * dw3.reshape(len(dw3),1)\n",
    "\n",
    "b1 -= 0.05 * db1 \n",
    "b2 -= 0.05 * db2\n",
    "b3 -= 0.05 * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.68349\n",
      "y prediction: [-0.72003]\n",
      "Input_layer w_gradient: [[-1.00e-05 -8.00e-05]\n",
      " [ 2.10e-04  1.13e-03]\n",
      " [ 9.00e-05  5.00e-04]\n",
      " [-1.30e-04 -6.80e-04]\n",
      " [-2.80e-04 -1.52e-03]\n",
      " [ 6.80e-04  3.67e-03]\n",
      " [-1.13e-03 -6.05e-03]\n",
      " [ 2.10e-04  1.12e-03]\n",
      " [ 1.10e-03  5.93e-03]\n",
      " [ 9.00e-04  4.85e-03]]\n",
      "Input_layer b_gradient: [-0.00017  0.00248  0.0011  -0.0015  -0.00336  0.00809 -0.01333  0.00246\n",
      "  0.01305  0.01068]\n",
      "Hidden_layer_1 weight_gradient: [[ 0.02321  0.02352  0.03394  0.0408   0.0046  -0.01027  0.03103 -0.01203\n",
      "  -0.0257   0.02161]\n",
      " [ 0.02342  0.02373  0.03424  0.04117  0.00464 -0.01036  0.03131 -0.01214\n",
      "  -0.02594  0.0218 ]\n",
      " [ 0.0252   0.02553  0.03684  0.0443   0.00499 -0.01115  0.03368 -0.01307\n",
      "  -0.02791  0.02346]\n",
      " [ 0.03736  0.03786  0.05463  0.06568  0.0074  -0.01653  0.04995 -0.01937\n",
      "  -0.04138  0.03478]\n",
      " [ 0.03658  0.03706  0.05348  0.0643   0.00724 -0.01618  0.0489  -0.01897\n",
      "  -0.04051  0.03405]\n",
      " [ 0.03411  0.03456  0.04988  0.05997  0.00675 -0.01509  0.0456  -0.01769\n",
      "  -0.03778  0.03176]\n",
      " [ 0.0398   0.04032  0.05819  0.06997  0.00788 -0.01761  0.0532  -0.02064\n",
      "  -0.04408  0.03705]\n",
      " [ 0.02897  0.02935  0.04236  0.05093  0.00574 -0.01282  0.03873 -0.01502\n",
      "  -0.03208  0.02697]\n",
      " [ 0.02532  0.02565  0.03702  0.04451  0.00501 -0.0112   0.03385 -0.01313\n",
      "  -0.02804  0.02357]\n",
      " [ 0.01899  0.01924  0.02776  0.03338  0.00376 -0.0084   0.02538 -0.00985\n",
      "  -0.02103  0.01768]]\n",
      "Hidden_layer_1 bias_gradient: [ 0.05976  0.06056  0.08739  0.10507  0.01183 -0.02644  0.07989 -0.03099\n",
      " -0.06619  0.05564]\n",
      "Hidden_layer_2 weight_gradient: [-0.73496 -0.54626 -0.87168 -0.9232  -0.63875 -0.87525 -0.79052 -0.6599\n",
      " -0.84924 -1.0612 ]\n",
      "Hidden_layer_2 bias_gradient: [-1.65347]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss = \" + str(round(loss_forward,5)))\n",
    "print(\"y prediction: \" + str(np.round(prediction_,5)))\n",
    "print(\"Input_layer w_gradient: \" + str(np.round(dw1.T,5)))\n",
    "print(\"Input_layer b_gradient: \" +  str(np.round(db1,5)))\n",
    "print(\"Hidden_layer_1 weight_gradient: \" + str(np.round(dw2,5)))\n",
    "print(\"Hidden_layer_1 bias_gradient: \" + str(np.round(db2,5)))\n",
    "print(\"Hidden_layer_2 weight_gradient: \" + str(np.round(dw3,5)))   \n",
    "print(\"Hidden_layer_2 bias_gradient: \" + str(np.round(db3,5)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Compare the two files torch_autograd.dat and my_autograd.dat and show that they give the same values up to numerical precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_deviation = 0.0\n",
      "prediction_deviation = 0.0\n",
      "input_layer w_deviation = 0.0\n",
      "input_layer b_deviation = 0.0\n",
      "hidden_layer_1 w_deviation = 0.0\n",
      "hidden_layer_1 b_deviation = 0.0\n",
      "hidden_layer_2 w_deviation = 0.0\n",
      "hidden_layer_2 b_deviation = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"loss_deviation = \" + str(np.round(loss.item(),5) - np.round(loss_forward.item(),5)))\n",
    "print(\"prediction_deviation = \" + str(np.round(prediction.item(),5) - np.round(prediction_.item(),5)))\n",
    "print(\"input_layer w_deviation = \" + str((np.round(input_layer.weight.grad.tolist(),4)-np.round(dw1.T,4)).sum()))\n",
    "print(\"input_layer b_deviation = \" + str((np.round(input_layer.bias.grad.tolist(),4)-np.round(db1,4)).sum()))\n",
    "print(\"hidden_layer_1 w_deviation = \" + str((np.round(hidden_layer1.weight.grad.tolist(),4)-np.round(dw2,4)).sum()))\n",
    "print(\"hidden_layer_1 b_deviation = \" + str((np.round(hidden_layer1.bias.grad.tolist(),4)-np.round(db2,4)).sum()))\n",
    "print(\"hidden_layer_2 w_deviation = \" + str((np.round(hidden_layer2.weight.grad.tolist(),4)-np.round(dw3,4)).sum()))\n",
    "print(\"hidden_layer_2 b_deviation = \" + str((np.round(hidden_layer2.bias.grad.tolist(),4)-np.round(db3,4)).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And I compare all the gradients of two methods, the result are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
